{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is a code to deal with the analysis from the artificial condensates:\n",
    "\n",
    "Needs: \n",
    "    1. Normalize the condensate mean intensities to the intensities at the dilute phase\n",
    "\n",
    "    2. Radius growth is normalized to nucleation time\n",
    "\n",
    "        For this, look at what particles are present at end time, then look for the earliest timeframe with the most in common\n",
    "        \n",
    "    3. Normalize total intensities to nucleation time (same as radius)\n",
    "\n",
    "First: load in the requirements,\n",
    "then load in the file locations and names\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#REQUIREMENTS\n",
    "\n",
    "import os\n",
    "import re\n",
    "import sys\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from scipy.stats import gaussian_kde\n",
    "import tifffile"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#DEFINITIONS:\n",
    "input_folder = '/Volumes/holtl02lab/holtl02labspace/Holt_Lab_Members/Nora_Holt/Live_Imaging_Other/250413_HeLa-2112-2113_2712-2713_6h-TFH/Results_FirstTFH'  # Replace with your input folder path\n",
    "output_folder = '/Volumes/holtl02lab/holtl02labspace/Holt_Lab_Members/Nora_Holt/Live_Imaging_Other/250413_HeLa-2112-2113_2712-2713_6h-TFH/Results_FirstTFH/output'  # Replace with your output folder path\n",
    "masks_folder = '/Volumes/holtl02lab/holtl02labspace/Holt_Lab_Members/Nora_Holt/Live_Imaging_Other/250413_HeLa-2112-2113_2712-2713_6h-TFH/First_TFH_Masks' #Replace with the fodler for your masks on the 1st timepoint/pre-TFH image\n",
    "output_filename_spots = 'all_data_spots'  # Name of output file\n",
    "output_filename_tracks = 'all_data_tracks'  # Name of output file\n",
    "group_names = [\"HeLa-2113_2712-2714_Dox\", \"HeLa-2112_2712-2714_Dox\", \"Untreated\"] #Names of groups to sort\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This code concatenates the _spots and _tracks files from TrackMate by their respective groups so that \n",
    "# The output is: you now have one big .csv file with all of the data from all the spots.csv (and another for tracks.csv) for condition 1, another for condition 2, \n",
    "\n",
    "\n",
    "def concatenate_and_sort_csv(input_folder, output_folder, output_filename_spots, output_filename_tracks, masks_folder):\n",
    "    def process_files(files, suffix, group_name, sort_columns, ascending_order):\n",
    "        dfs = []\n",
    "        for file in files:\n",
    "            if group_name not in file:\n",
    "                continue\n",
    "            \n",
    "            file_path = os.path.join(input_folder, file)\n",
    "\n",
    "            try:\n",
    "                df = pd.read_csv(file_path, header=0)\n",
    "            except Exception as e:\n",
    "                print(f\"Error reading {file_path}: {e}\")\n",
    "                continue\n",
    "\n",
    "            core_filename = file[:-len(suffix)]\n",
    "            df.insert(0, 'original_filename', core_filename)\n",
    "\n",
    "            if 'TRACK_ID' in df.columns:\n",
    "                df['unique_name'] = df['original_filename'] + \"_\" + df['TRACK_ID'].astype(str)\n",
    "            else:\n",
    "                print(f\"Warning: 'TRACK_ID' column missing in {file}\")\n",
    "                df['unique_name'] = df['original_filename']\n",
    "\n",
    "            cols = list(df.columns)\n",
    "            cols.remove('unique_name')\n",
    "            cols.insert(1, 'unique_name')\n",
    "            df = df[cols]\n",
    "\n",
    "            if 'LABEL' in df.columns:\n",
    "                df = df.drop(columns=['LABEL'])\n",
    "\n",
    "            # Compute mean intensity\n",
    "            try:\n",
    "                img_path = os.path.join(masks_folder, f\"{core_filename}.tif\")\n",
    "                mask_path = os.path.join(masks_folder, f\"{core_filename}_MASK.tif\")\n",
    "\n",
    "                image = tifffile.imread(img_path)\n",
    "                mask = tifffile.imread(mask_path)\n",
    "\n",
    "                if image.shape != mask.shape:\n",
    "                    print(f\"Shape mismatch in {core_filename}: image {image.shape}, mask {mask.shape}\")\n",
    "                    mean_intensity = np.nan\n",
    "                else:\n",
    "                    roi_pixels = image[mask > 0]\n",
    "                    mean_intensity = roi_pixels.mean() if roi_pixels.size > 0 else np.nan\n",
    "            except Exception as e:\n",
    "                print(f\"Error reading image or mask for {core_filename}: {e}\")\n",
    "                mean_intensity = np.nan\n",
    "\n",
    "            df[\"Mean_Intensity_t0\"] = mean_intensity\n",
    "            \n",
    "            # Move \"Mean_Intensity_t0\" to right after \"unique_name\"\n",
    "            cols = list(df.columns)\n",
    "            cols.remove(\"Mean_Intensity_t0\")\n",
    "            insert_idx = cols.index(\"unique_name\") + 1\n",
    "            cols.insert(insert_idx, \"Mean_Intensity_t0\")\n",
    "            df = df[cols]\n",
    "            dfs.append(df)\n",
    "\n",
    "        if not dfs:\n",
    "            print(f\"No matching files for group '{group_name}' and suffix '{suffix}'\")\n",
    "            return pd.DataFrame()  # return empty dataframe\n",
    "\n",
    "        result = pd.concat(dfs, ignore_index=True)\n",
    "        for col in result.columns:\n",
    "            if col not in ['original_filename', 'unique_name']:\n",
    "                result[col] = pd.to_numeric(result[col], errors='coerce')\n",
    "\n",
    "        result = result.sort_values(by=sort_columns, ascending=ascending_order)\n",
    "        return result\n",
    "\n",
    "    for group_name in [\"HeLa-2113_2712-2714_Dox\", \"HeLa-2112_2712-2714_Dox\", \"Untreated\"]:\n",
    "        spots_files = [f for f in os.listdir(input_folder) if f.endswith('_spots.csv')]\n",
    "        tracks_files = [f for f in os.listdir(input_folder) if f.endswith('_tracks.csv')]\n",
    "\n",
    "        if spots_files:\n",
    "            spots_result = process_files(spots_files, '_spots.csv', group_name, ['unique_name', 'FRAME'], [True, True])\n",
    "            if not spots_result.empty:\n",
    "                output_file_spots = os.path.join(output_folder, f\"{output_filename_spots}_{group_name}.csv\")\n",
    "                spots_result.to_csv(output_file_spots, index=False, header=True)\n",
    "                print(f\"✅ Saved spots CSV: {output_file_spots}\")\n",
    "\n",
    "        if tracks_files:\n",
    "            tracks_result = process_files(tracks_files, '_tracks.csv', group_name, ['unique_name'], [True])\n",
    "            if not tracks_result.empty:\n",
    "                output_file_tracks = os.path.join(output_folder, f\"{output_filename_tracks}_{group_name}.csv\")\n",
    "                tracks_result.to_csv(output_file_tracks, index=False, header=True)\n",
    "                print(f\"✅ Saved tracks CSV: {output_file_tracks}\")\n",
    "\n",
    "concatenate_and_sort_csv(input_folder, output_folder, output_filename_spots, output_filename_tracks, masks_folder)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that I have the all_data.csv files, I need to be able to skip the concatenate_and_sort script if I want to reanalyze existing data in the future:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_spots_dox_2113 = pd.read_csv(os.path.join(output_folder, f\"{output_filename_spots}_HeLa-2113_2712-2714_Dox.csv\"), header=0)\n",
    "df_tracks_dox_2113 = pd.read_csv(os.path.join(output_folder, f\"{output_filename_tracks}_HeLa-2113_2712-2714_Dox.csv\"), header=0)\n",
    "df_spots_dox_2112 = pd.read_csv(os.path.join(output_folder, f\"{output_filename_spots}_HeLa-2112_2712-2714_Dox.csv\"), header=0)\n",
    "df_tracks_dox_2112 = pd.read_csv(os.path.join(output_folder, f\"{output_filename_tracks}_HeLa-2112_2712-2714_Dox.csv\"), header=0)\n",
    "df_spots_un = pd.read_csv(os.path.join(output_folder, f\"{output_filename_spots}_Untreated.csv\"), header=0)\n",
    "df_tracks_un = pd.read_csv(os.path.join(output_folder, f\"{output_filename_tracks}_Untreated.csv\"), header=0)\n",
    "\n",
    "print(df_spots_dox_2113.columns.tolist())\n",
    "print(df_tracks_dox_2113.columns.tolist())\n",
    "print(df_spots_dox_2112.columns.tolist())\n",
    "print(df_tracks_dox_2112.columns.tolist())\n",
    "print(df_spots_un.columns.tolist())\n",
    "print(df_tracks_un.columns.tolist())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Assign one comparison at a time:\n",
    "\n",
    "df_tracks_dox = df_tracks_dox_2112"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here I want to graph the nucleation points for all tracks identified in the .csv files in each group"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove rows where 'TRACK_INDEX' is blank\n",
    "df_tracks_dox_filtered = df_tracks_dox.dropna(subset=['TRACK_INDEX'])\n",
    "df_tracks_un_filtered = df_tracks_un.dropna(subset=['TRACK_INDEX'])\n",
    "\n",
    "# Save the filtered data to new CSV files\n",
    "df_tracks_dox_filtered.to_csv(os.path.join(output_folder, f\"{output_filename_tracks}_Dox-filtered.csv\"), index=False)\n",
    "df_tracks_un_filtered.to_csv(os.path.join(output_folder, f\"{output_filename_tracks}_Untreated-filtered.csv\"), index=False)\n",
    "\n",
    "# Modify 'TRACK_START' by dividing by 60, rounding down, and keeping 0 as 0\n",
    "#df_tracks_dox_filtered['TRACK_START'] = df_tracks_dox_filtered['TRACK_START'].apply(lambda x: max(0, np.floor(x / 60)))\n",
    "#df_tracks_un_filtered['TRACK_START'] = df_tracks_un_filtered['TRACK_START'].apply(lambda x: max(0, np.floor(x / 60)))\n",
    "\n",
    "# Define bins (1-minute bins)\n",
    "bin_width = 1\n",
    "max_bin = max(df_tracks_dox_filtered['TRACK_START'].max(), df_tracks_un_filtered['TRACK_START'].max()) + bin_width\n",
    "bins = np.arange(0, max_bin, bin_width)\n",
    "\n",
    "# Compute histogram counts\n",
    "dox_counts, _ = np.histogram(df_tracks_dox_filtered['TRACK_START'], bins=bins)\n",
    "un_counts, _ = np.histogram(df_tracks_un_filtered['TRACK_START'], bins=bins)\n",
    "\n",
    "# Create DataFrame for output\n",
    "histogram_df = pd.DataFrame({\n",
    "    \"TRACK_START\": bins[:-1],  # Bin start values\n",
    "    \"TRACK_START\": bins[:-1],  # Same as TRACK_START\n",
    "    \"NUCLEATION_EVENTS_Dox\": dox_counts,\n",
    "    \"NUCLEATION_EVENTS_Untreated\": un_counts\n",
    "})\n",
    "\n",
    "# Save to CSV\n",
    "output_csv_path = os.path.join(output_folder, \"nucleation_events.csv\")\n",
    "histogram_df.to_csv(output_csv_path, index=False)\n",
    "\n",
    "print(f\"Saved nucleation event counts to {output_csv_path}\")\n",
    "\n",
    "# Plot the density of 'TRACK_START' for Dox and Untreated groups\n",
    "plt.figure(figsize=(10, 6))\n",
    "\n",
    "# Plot density for Dox group\n",
    "sns.kdeplot(df_tracks_dox_filtered['TRACK_START'], color='darkblue', label='Dox', shade=True, alpha=0.5)\n",
    "\n",
    "# Plot density for Untreated group\n",
    "sns.kdeplot(df_tracks_un_filtered['TRACK_START'], color='darkred', label='Untreated', shade=True, alpha=0.5)\n",
    "\n",
    "# Labels and title\n",
    "plt.xlabel(\"Nucleation time (in minutes)\")\n",
    "plt.ylabel(\"Density\")\n",
    "plt.title(\"Density Plot of TRACK_START for Dox and Untreated Groups\")\n",
    "plt.legend()\n",
    "plt.grid(True, linestyle=\"--\", alpha=0.5)\n",
    "\n",
    "plt.savefig(f\"{output_folder}/nucleation_density.png\", bbox_inches=\"tight\")\n",
    "# Show the plot\n",
    "plt.show()\n",
    "\n",
    "plt.figure(figsize=(10, 6))\n",
    "\n",
    "# Define bins (1-minute bins)\n",
    "bin_width = 1\n",
    "bins = np.arange(0, max(df_tracks_dox_filtered['TRACK_START'].max(), df_tracks_un_filtered['TRACK_START'].max()) + bin_width, bin_width)\n",
    "\n",
    "# Plot histogram for Dox\n",
    "sns.histplot(df_tracks_dox_filtered['TRACK_START'], bins=bins, color='blue', label='Dox', alpha=0.6, kde=False, stat='count')\n",
    "\n",
    "# Plot histogram for Untreated\n",
    "sns.histplot(df_tracks_un_filtered['TRACK_START'], bins=bins, color='red', label='Untreated', alpha=0.6, kde=False, stat='count')\n",
    "\n",
    "# Labels and title\n",
    "plt.xlabel(\"Nucleation time (in minutes)\")\n",
    "plt.ylabel(\"Track count\")\n",
    "plt.title(\"Histogram of Track Nucleation Times\")\n",
    "plt.legend()\n",
    "plt.grid(True, linestyle=\"--\", alpha=0.5)\n",
    "\n",
    "# Save and show\n",
    "plt.savefig(f\"{output_folder}/nucleation_histogram.png\", bbox_inches=\"tight\")\n",
    "plt.show()\n",
    "\n",
    "plt.figure(figsize=(10, 6))\n",
    "\n",
    "# Define bins\n",
    "bin_width = 1\n",
    "bins = np.arange(0, max(df_tracks_dox_filtered['TRACK_START'].max(), df_tracks_un_filtered['TRACK_START'].max()) + bin_width, bin_width)\n",
    "\n",
    "# Histogram: Dox\n",
    "sns.histplot(df_tracks_dox_filtered['TRACK_START'], bins=bins, color='blue', label='Dox (count)', alpha=0.4, kde=False, stat='count')\n",
    "\n",
    "# Histogram: Untreated\n",
    "sns.histplot(df_tracks_un_filtered['TRACK_START'], bins=bins, color='red', label='Untreated (count)', alpha=0.4, kde=False, stat='count')\n",
    "\n",
    "# KDE: manually compute and scale density × 100\n",
    "from scipy.stats import gaussian_kde\n",
    "\n",
    "# KDE for Dox\n",
    "dox_kde = gaussian_kde(df_tracks_dox_filtered['TRACK_START'])\n",
    "x_vals = np.linspace(0, max(bins), 500)\n",
    "dox_density = dox_kde(x_vals) * 100  # scale KDE\n",
    "\n",
    "# KDE for Untreated\n",
    "un_kde = gaussian_kde(df_tracks_un_filtered['TRACK_START'])\n",
    "un_density = un_kde(x_vals) * 100  # scale KDE\n",
    "\n",
    "# Plot scaled KDE curves\n",
    "plt.plot(x_vals, dox_density, color='darkblue', label='Dox (density ×100)', linewidth=2)\n",
    "plt.plot(x_vals, un_density, color='darkred', label='Untreated (density ×100)', linewidth=2)\n",
    "\n",
    "# Labels and title\n",
    "plt.xlabel(\"Nucleation time (in minutes)\")\n",
    "plt.ylabel(\"Track count / Scaled density\")\n",
    "plt.title(\"Track Nucleation Time: Histogram + KDE Overlay (Density ×100)\")\n",
    "plt.legend()\n",
    "plt.grid(True, linestyle=\"--\", alpha=0.5)\n",
    "\n",
    "# Save and show\n",
    "plt.savefig(f\"{output_folder}/nucleation_histogram_kde_scaled.png\", bbox_inches=\"tight\")\n",
    "plt.show()\n",
    "\n",
    "plt.figure(figsize=(10, 6))\n",
    "\n",
    "# Scatter plot for Dox\n",
    "sns.scatterplot(\n",
    "    data=df_tracks_dox_filtered,\n",
    "    x=\"TRACK_START\",\n",
    "    y=\"Mean_Intensity_t0\",\n",
    "    color=\"blue\",\n",
    "    label=\"Dox\",\n",
    "    alpha=0.6\n",
    ")\n",
    "\n",
    "# Scatter plot for Untreated\n",
    "sns.scatterplot(\n",
    "    data=df_tracks_un_filtered,\n",
    "    x=\"TRACK_START\",\n",
    "    y=\"Mean_Intensity_t0\",\n",
    "    color=\"red\",\n",
    "    label=\"Untreated\",\n",
    "    alpha=0.6\n",
    ")\n",
    "\n",
    "plt.xlabel(\"Nucleation time (in minutes)\")\n",
    "plt.ylabel(\"Mean Intensity Dilute Phase\")\n",
    "plt.title(\"Nucleation time vs. Mean Intensity Dilute Phase\")\n",
    "plt.legend()\n",
    "plt.grid(True, linestyle=\"--\", alpha=0.5)\n",
    "\n",
    "# Save and show\n",
    "plt.savefig(f\"{output_folder}/mean-intensity-t0_vs_time.png\", bbox_inches=\"tight\")\n",
    "plt.show()\n",
    "\n",
    "plt.figure(figsize=(8, 6))\n",
    "\n",
    "# Combine the two filtered DataFrames and add a group column\n",
    "df_tracks_dox_filtered[\"Group\"] = \"Dox\"\n",
    "df_tracks_un_filtered[\"Group\"] = \"Untreated\"\n",
    "df_combined = pd.concat([df_tracks_dox_filtered, df_tracks_un_filtered])\n",
    "\n",
    "# Boxplot\n",
    "sns.boxplot(\n",
    "    data=df_combined,\n",
    "    x=\"Group\",\n",
    "    y=\"Mean_Intensity_t0\",\n",
    "    palette={\"Dox\": \"blue\", \"Untreated\": \"red\"},\n",
    "    showcaps=True,\n",
    "    boxprops={\"facecolor\": \"none\", \"edgecolor\": \"black\"},\n",
    "    whiskerprops={\"color\": \"black\"},\n",
    "    medianprops={\"color\": \"black\"}\n",
    ")\n",
    "\n",
    "# Overlay points\n",
    "sns.stripplot(\n",
    "    data=df_combined,\n",
    "    x=\"Group\",\n",
    "    y=\"Mean_Intensity_t0\",\n",
    "    jitter=True,\n",
    "    alpha=0.5,\n",
    "    palette={\"Dox\": \"blue\", \"Untreated\": \"red\"}\n",
    ")\n",
    "\n",
    "# Labels and title\n",
    "plt.ylabel(\"Mean Intensity t0\")\n",
    "plt.title(\"Mean Intensity of Dilute Phase (t0) by Group\")\n",
    "plt.grid(True, linestyle=\"--\", alpha=0.5)\n",
    "\n",
    "# Save and show\n",
    "plt.savefig(f\"{output_folder}/mean_intensity_t0_boxplot.png\", bbox_inches=\"tight\")\n",
    "plt.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For the next set of cells, I want to be able to run these scripts without running the nucleation points.\n",
    "\n",
    "TO FILTER BASED ON WHAT SPOTS ARE PRESENT WITHIN A CERTAIN RANGE (ex: 11-30, 0-15, etc):\n",
    "\n",
    "1. I have column \"FRAME\". I need to look for each instance where FRAME = {lst}, then identify the \"unique_name\" for each instance of {lst}. \n",
    "\n",
    "2. Then I need to create a subset df (df_subset_lst_{group}) that has all rows & columns for each \"unique_name\" pulled from (1).\n",
    "\n",
    "3. Then I need to create two additional subsets with the following conditions:\n",
    "\n",
    "    ONE:\n",
    "    \n",
    "    4. Starting from df_subset_lst_{group} I need a subset for all \"unique_name\" values with \"FRAME in range {first} to {lst}\n",
    "    \n",
    "    5. I need to graph the number of spots vs. time and the total area of spots vs. time in range {first}-{lst}\n",
    "\n",
    "\n",
    "    TWO:\n",
    "    \n",
    "    4. Starting from df_subset_lst_{group} I need to find the value counts to determine the mode frame number before {lst}. This is {nuc_frame}. \n",
    "    \n",
    "    5. I need to run a check for value and frame counts in case I want to manually alter the {nuc_frame}\n",
    "    \n",
    "    6. Then I need to normalize the values by the values at frame {nuc_frame}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#This creates a subset for 30 as the endpt. if I want to go forward only with tracks that are present at the end of the image.\n",
    "first = 0\n",
    "lst =  100 #this is the last frame number for the range normalization\n",
    "\n",
    "# Loop through each group\n",
    "df_subset_lst_dox = None\n",
    "df_subset_lst_un = None\n",
    "\n",
    "for group_name in group_names:\n",
    "    # Read the respective CSV files for each group\n",
    "    df_spots = pd.read_csv(os.path.join(output_folder, f\"{output_filename_spots}_{group_name}.csv\"), header=0)\n",
    "    df_tracks = pd.read_csv(os.path.join(output_folder, f\"{output_filename_tracks}_{group_name}.csv\"), header=0)\n",
    "    \n",
    "    # Get the unique names for the specified frame (lst)\n",
    "    unique_names_lst = df_spots[df_spots['FRAME'] == lst]['unique_name']\n",
    "    print(f\"Unique names for {group_name} at frame {lst}:\", unique_names_lst)\n",
    "    \n",
    "    # Create a subset dataframe based on the unique names\n",
    "    df_subset_lst = df_spots[df_spots['unique_name'].isin(unique_names_lst)]\n",
    "    \n",
    "    # Store the subset result in the corresponding variable\n",
    "    if group_name == \"HeLa-2112_2712-2714_Dox\":\n",
    "        df_subset_lst_dox = df_subset_lst\n",
    "    else:\n",
    "        df_subset_lst_un = df_subset_lst"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a function to filter and save the dataframe, and define the max intensity for t0 dilute phase\n",
    "def process_and_save_subset(df_subset_lst, group_name, first, lst, output_folder, max_intensity=400):\n",
    "    # Filter for frames within the range [first - 1, lst]\n",
    "    df_subset_lst = df_subset_lst[(df_subset_lst[\"FRAME\"] >= (first - 1)) & \n",
    "                                  (df_subset_lst[\"FRAME\"] <= lst)]\n",
    "\n",
    "    # Optional filter for maximum Mean_Intensity_t0\n",
    "    if max_intensity is not None:\n",
    "        df_subset_lst = df_subset_lst[df_subset_lst[\"Mean_Intensity_t0\"] <= max_intensity]\n",
    "\n",
    "    # Add COUNT column: sequential numbering within each unique_name\n",
    "    df_subset_lst[\"COUNT\"] = df_subset_lst[\"FRAME\"] - first\n",
    "\n",
    "    # Save to CSV\n",
    "    filename = f\"subset_spots_{group_name}_{first}-{lst}.csv\"\n",
    "    output_path = os.path.join(output_folder, filename)\n",
    "    df_subset_lst.to_csv(output_path, index=False)\n",
    "\n",
    "    # Return the processed dataframe\n",
    "    return df_subset_lst\n",
    "\n",
    "# Process for Dox\n",
    "df_subset_range_dox = process_and_save_subset(df_subset_lst_dox, \"Dox\", first, lst, output_folder)\n",
    "\n",
    "# Process for Untreated\n",
    "df_subset_range_un = process_and_save_subset(df_subset_lst_un, \"Untreated\", first, lst, output_folder)\n",
    "\n",
    "\n",
    "#sanity check for filtration:\n",
    "plt.figure(figsize=(8, 6))\n",
    "\n",
    "# Combine the two filtered DataFrames and add a group column\n",
    "df_subset_range_dox[\"Group\"] = \"Dox\"\n",
    "df_subset_range_un[\"Group\"] = \"Untreated\"\n",
    "df_combined = pd.concat([df_subset_range_dox, df_subset_range_un])\n",
    "\n",
    "# Boxplot\n",
    "sns.boxplot(\n",
    "    data=df_combined,\n",
    "    x=\"Group\",\n",
    "    y=\"Mean_Intensity_t0\",\n",
    "    palette={\"Dox\": \"blue\", \"Untreated\": \"red\"},\n",
    "    showcaps=True,\n",
    "    boxprops={\"facecolor\": \"none\", \"edgecolor\": \"black\"},\n",
    "    whiskerprops={\"color\": \"black\"},\n",
    "    medianprops={\"color\": \"black\"}\n",
    ")\n",
    "\n",
    "# Overlay points\n",
    "sns.stripplot(\n",
    "    data=df_combined,\n",
    "    x=\"Group\",\n",
    "    y=\"Mean_Intensity_t0\",\n",
    "    jitter=True,\n",
    "    alpha=0.5,\n",
    "    palette={\"Dox\": \"blue\", \"Untreated\": \"red\"}\n",
    ")\n",
    "\n",
    "# Labels and title\n",
    "plt.ylabel(\"Mean Intensity t0\")\n",
    "plt.title(\"Mean Intensity of Dilute Phase (t0) by Group\")\n",
    "plt.grid(True, linestyle=\"--\", alpha=0.5)\n",
    "\n",
    "# Save and show\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Graph the number of spots vs. time and the total area of spots vs. time in range {first}-{lst}\n",
    "\n",
    "# Define colors for Dox and Untreated groups\n",
    "dox_light = \"lightblue\"\n",
    "dox_dark = \"darkblue\"\n",
    "untreated_light = \"lightcoral\"\n",
    "untreated_dark = \"darkred\"\n",
    "\n",
    "# Define output CSV file path\n",
    "filename = f\"spots_area_{first}-{lst}.csv\"\n",
    "csv_output_path = os.path.join(output_folder, filename)\n",
    "\n",
    "# Initialize an empty list to collect data for CSV\n",
    "csv_data = []\n",
    "\n",
    "# Define variables and their shorthand names for plotting\n",
    "variables = {\n",
    "    \"unique_track_ids\": \"spots-count\",\n",
    "    \"total_area\": \"total-area\",\n",
    "}\n",
    "y_labels = {\n",
    "    \"unique_track_ids\": \"Number of Condensates\",\n",
    "    \"total_area\": \"Total Condensate Area\",\n",
    "}\n",
    "\n",
    "# Loop over each variable (Unique TRACK_ID count and Total Area)\n",
    "for var, shorthand in variables.items():\n",
    "    ## **Plot 1: Individual Traces + Group Averages**\n",
    "    plt.figure(figsize=(8, 6))\n",
    "\n",
    "    # Group by 'original_filename' and 'FRAME', then calculate metrics for Dox\n",
    "    grouped_dox = df_subset_range_dox.groupby(['original_filename', 'FRAME']).agg(\n",
    "        unique_track_ids=('TRACK_ID', 'nunique'),\n",
    "        total_area=('AREA', 'sum')\n",
    "    ).reset_index()\n",
    "\n",
    "    # Plot individual traces for Dox (light color)\n",
    "    for _, group in grouped_dox.groupby(\"original_filename\"):\n",
    "        plt.plot(group[\"FRAME\"], group[var], color=dox_light, alpha=0.3, linewidth=1)\n",
    "\n",
    "    # Group by 'FRAME' for the average and SEM of Dox\n",
    "    dox_avg = grouped_dox.groupby(\"FRAME\").agg(\n",
    "        avg_y=(var, 'mean'),\n",
    "        sem_y=(var, lambda x: np.std(x) / np.sqrt(len(x)))\n",
    "    ).reset_index()\n",
    "\n",
    "    # Plot group averages for Dox (dark color)\n",
    "    plt.plot(dox_avg[\"FRAME\"], dox_avg[\"avg_y\"], color=dox_dark, marker=\"o\", linestyle=\"-\", linewidth=2, label=\"Dox (Mean)\")\n",
    "\n",
    "    # Group by 'original_filename' and 'FRAME', then calculate metrics for Untreated\n",
    "    grouped_un = df_subset_range_un.groupby(['original_filename', 'FRAME']).agg(\n",
    "        unique_track_ids=('TRACK_ID', 'nunique'),\n",
    "        total_area=('AREA', 'sum')\n",
    "    ).reset_index()\n",
    "\n",
    "    # Plot individual traces for Untreated (light color)\n",
    "    for _, group in grouped_un.groupby(\"original_filename\"):\n",
    "        plt.plot(group[\"FRAME\"], group[var], color=untreated_light, alpha=0.3, linewidth=1)\n",
    "\n",
    "    # Group by 'FRAME' for the average and SEM of Untreated\n",
    "    untreated_avg = grouped_un.groupby(\"FRAME\").agg(\n",
    "        avg_y=(var, 'mean'),\n",
    "        sem_y=(var, lambda x: np.std(x) / np.sqrt(len(x)))\n",
    "    ).reset_index()\n",
    "\n",
    "    # Combine both groups\n",
    "    df_combined = pd.concat([grouped_dox, grouped_un], ignore_index=True)\n",
    "\n",
    "    # Compute overall averages per FRAME across both groups\n",
    "    df_avg = df_combined.groupby(\"FRAME\").agg(\n",
    "        avg_spots_count=('unique_track_ids', 'mean'),\n",
    "        avg_total_area=('total_area', 'mean')\n",
    "    ).reset_index()\n",
    "\n",
    "    # Merge the averages into the combined dataframe\n",
    "    df_combined = df_combined.merge(df_avg, on=\"FRAME\", how=\"left\")\n",
    "\n",
    "    # Select and reorder columns\n",
    "    df_combined = df_combined[[\n",
    "        \"original_filename\", \"FRAME\", \"unique_track_ids\", \"total_area\",\n",
    "        \"avg_spots_count\", \"avg_total_area\"\n",
    "    ]]\n",
    "\n",
    "    # Rename columns for clarity\n",
    "    df_combined.columns = [\n",
    "        \"original_filename\", \"FRAME\", \"spots_count\", \"total_area\",\n",
    "        \"avg_spots_count-perframe\", \"avg_total_area-perframe\"\n",
    "    ]\n",
    "\n",
    "    # Append to CSV data list\n",
    "    csv_data.append(df_combined)\n",
    "\n",
    "   # Plot group averages for Untreated (dark color)\n",
    "    plt.plot(untreated_avg[\"FRAME\"], untreated_avg[\"avg_y\"], color=untreated_dark, marker=\"o\", linestyle=\"-\", linewidth=2, label=\"Untreated (Mean)\")\n",
    "\n",
    "    # Labels and legend\n",
    "    plt.xlabel(\"t/t0\")\n",
    "    plt.ylabel(y_labels[var])\n",
    "    plt.title(f\"Individual Traces and Group Averages ({y_labels[var]})\")\n",
    "    plt.legend()\n",
    "    plt.grid(True, linestyle=\"--\", alpha=0.5)\n",
    "\n",
    "    # Save plot\n",
    "    plt.savefig(f\"{output_folder}/{shorthand}-all_{first}-{lst}.png\", bbox_inches=\"tight\")\n",
    "    plt.show()\n",
    "\n",
    "    ## **Plot 2: Mean with SEM Error Bars**\n",
    "    plt.figure(figsize=(8, 6))\n",
    "\n",
    "    # Plot group means with error bars for Dox\n",
    "    plt.errorbar(dox_avg[\"FRAME\"], dox_avg[\"avg_y\"], yerr=dox_avg[\"sem_y\"], fmt=\"o-\", color=dox_dark, capsize=5, label=\"Dox (Mean ± SEM)\")\n",
    "\n",
    "    # Plot group means with error bars for Untreated\n",
    "    plt.errorbar(untreated_avg[\"FRAME\"], untreated_avg[\"avg_y\"], yerr=untreated_avg[\"sem_y\"], fmt=\"o-\", color=untreated_dark, capsize=5, label=\"Untreated (Mean ± SEM)\")\n",
    "\n",
    "    # Labels and legend\n",
    "    plt.xlabel(\"t/t0\")\n",
    "    plt.ylabel(y_labels[var])\n",
    "    plt.title(f\"Group Averages with SEM Error Bars ({y_labels[var]})\")\n",
    "    plt.legend()\n",
    "    plt.grid(True, linestyle=\"--\", alpha=0.5)\n",
    "\n",
    "    # Save plot\n",
    "    plt.savefig(f\"{output_folder}/{shorthand}-avg_{first}-{lst}.png\", bbox_inches=\"tight\")\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "# Concatenate all variables into one DataFrame\n",
    "df_final = pd.concat(csv_data, ignore_index=True)\n",
    "\n",
    "# Save to CSV\n",
    "df_final.to_csv(csv_output_path, index=False)\n",
    "\n",
    "print(f\"CSV file saved: {csv_output_path}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "NOW I NEED TO NORMALIZE:\n",
    "\n",
    "Normalize to the lowest frame number at least 2/3 of the total number of spots at the endpoint\n",
    "\n",
    "For 30 in this dataset: it would be frame 11 because frame 11 has 394 spots, and 394 > 2/3(581)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_subset_lst_dox['FRAME'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_subset_lst_un['FRAME'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the value counts of FRAME for Dox and Untreated\n",
    "frame_counts_dox = df_subset_lst_dox['FRAME'].value_counts().sort_index()\n",
    "frame_counts_un = df_subset_lst_un['FRAME'].value_counts().sort_index()\n",
    "\n",
    "# Find the highest frame count for both groups\n",
    "max_count_dox = frame_counts_dox.max()\n",
    "max_count_un = frame_counts_un.max()\n",
    "\n",
    "# Determine the lower max count between the two groups\n",
    "lower_max_count = min(max_count_dox, max_count_un)\n",
    "higher_max_count =  max(max_count_dox, max_count_un)\n",
    "\n",
    "# Compute 2/3 of the lower max count\n",
    "threshold = lower_max_count * (2 / 3)\n",
    "threshhold_2 = higher_max_count * (2 / 3)\n",
    "\n",
    "# Find the lowest FRAME number where count >= threshold for both groups\n",
    "lowest_frame_dox = frame_counts_dox[frame_counts_dox >= threshold].index.min()\n",
    "lowest_frame_un = frame_counts_un[frame_counts_un >= threshold].index.min()\n",
    "\n",
    "# The final lowest frame is the minimum of the two lowest frames\n",
    "nuc_frame = min(int(lowest_frame_dox), int(lowest_frame_un))\n",
    "\n",
    "# Print results\n",
    "print(f\"Highest frame count for Dox: {max_count_dox}\")\n",
    "print(f\"Highest frame count for Untreated: {max_count_un}\")\n",
    "print(f\"Threshold (2/3 of the lower max count): {threshold:.2f}\")\n",
    "print(f\"Threshold (2/3 of the lower max count): {threshhold_2:.2f}\")\n",
    "print(f\"Lowest frame with count above threshold (for both groups): {nuc_frame}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nuc_frame = 10 #use this to manually assign the frame range if needed\n",
    "lst = 50 #reassign last"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a function to filter and save the dataframe\n",
    "def process_and_save_subset(df_subset_lst, group_name, nuc_frame, lst, output_folder):\n",
    "    # Ensure the dataframe is not None\n",
    "    if df_subset_lst is None:\n",
    "        raise ValueError(f\"Dataframe for {group_name} is None, please check data loading.\")\n",
    "    \n",
    "    # Print debug information to verify dataframe contents before processing\n",
    "    print(f\"Processing {group_name} dataframe:\")\n",
    "    print(df_subset_lst.head())  # Show first few rows for verification\n",
    "\n",
    "    # Filter for frames within the range [0, lst]\n",
    "    df_subset_lst = df_subset_lst[(df_subset_lst[\"FRAME\"] >= (nuc_frame - 1)) & \n",
    "                                  (df_subset_lst[\"FRAME\"] <= lst)]\n",
    "\n",
    "    # Compute COUNT relative to nuc_frame\n",
    "    df_subset_lst[\"COUNT\"] = df_subset_lst[\"FRAME\"] - nuc_frame + 1\n",
    "\n",
    "    # Save to CSV\n",
    "    filename = f\"subset_spots_{group_name}_{nuc_frame}-{lst}.csv\"\n",
    "    output_path = os.path.join(output_folder, filename)\n",
    "    df_subset_lst.to_csv(output_path, index=False)\n",
    "\n",
    "    # Return the processed dataframe\n",
    "    return df_subset_lst\n",
    "\n",
    "# Process for Dox\n",
    "df_subset_lst_dox = process_and_save_subset(df_subset_lst_dox, \"Dox\", nuc_frame, lst, output_folder)\n",
    "\n",
    "# Process for Untreated\n",
    "df_subset_lst_un = process_and_save_subset(df_subset_lst_un, \"Untreated\", nuc_frame, lst, output_folder)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a function to normalize data for a given group (Dox or Untreated)\n",
    "def normalize_group(df_subset_lst, group_name, nuc_frame, lst, output_folder):\n",
    "    df_subset_lst = df_subset_lst.copy()\n",
    "    print(\"Before apply:\", df_subset_lst.shape)\n",
    "\n",
    "    # --- STEP 1: Normalize intensity columns to Mean_Intensity_t0 ---\n",
    "    intensity_cols = [\"MEAN_INTENSITY_CH1\", \"MEDIAN_INTENSITY_CH1\", \"TOTAL_INTENSITY_CH1\"]\n",
    "    for col in intensity_cols:\n",
    "        df_subset_lst[f\"{col}_t0NORM\"] = df_subset_lst[col] / df_subset_lst[\"Mean_Intensity_t0\"]\n",
    "\n",
    "    # --- STEP 2: Normalize RADIUS, AREA, PERIMETER to their value at nuc_frame ---\n",
    "    norm_cols_direct = [\"RADIUS\", \"AREA\", \"PERIMETER\"]\n",
    "\n",
    "    # Keep only rows with nuc_frame to use as reference\n",
    "    valid_names = df_subset_lst[df_subset_lst[\"FRAME\"] == nuc_frame][\"unique_name\"].unique()\n",
    "    filtered_df = df_subset_lst[df_subset_lst[\"unique_name\"].isin(valid_names)].copy()\n",
    "    if \"unique_name\" not in filtered_df.columns:\n",
    "        raise KeyError(\"'unique_name' not in DataFrame columns:\\n\" + str(filtered_df.columns))\n",
    "\n",
    "    # Get reference values at nuc_frame for direct normalization\n",
    "    ref_direct = filtered_df[filtered_df[\"FRAME\"] == nuc_frame].set_index(\"unique_name\")[norm_cols_direct]\n",
    "\n",
    "    def normalize_direct(group):\n",
    "        group = group.copy()\n",
    "        if nuc_frame in group[\"FRAME\"].values:\n",
    "            ref = ref_direct.loc[group[\"unique_name\"].iloc[0]]\n",
    "            for col in norm_cols_direct:\n",
    "                group[f\"{col}_NORM\"] = group[col] / ref[col]\n",
    "        else:\n",
    "            print(f\"Skipping direct norm for {group['unique_name'].iloc[0]}\")\n",
    "        return group\n",
    "\n",
    "\n",
    "    filtered_df = filtered_df.groupby(\"unique_name\",group_keys=False).apply(normalize_direct)\n",
    "    print(\"After normalization:\", filtered_df.shape)\n",
    "\n",
    "    # --- STEP 3: Normalize *_t0norm columns to their value at nuc_frame ---\n",
    "    norm_cols_t0norm = [f\"{col}_t0NORM\" for col in intensity_cols]\n",
    "    ref_t0norm = filtered_df[filtered_df[\"FRAME\"] == nuc_frame].set_index(\"unique_name\")[norm_cols_t0norm]\n",
    "\n",
    "    def normalize_t0(group):\n",
    "        group = group.copy()\n",
    "        if nuc_frame in group[\"FRAME\"].values:\n",
    "            ref = ref_t0norm.loc[group[\"unique_name\"].iloc[0]]\n",
    "            for col in norm_cols_t0norm:\n",
    "                group[col.replace(\"_t0NORM\", \"_NORM\")] = group[col] / ref[col]\n",
    "        else:\n",
    "            print(f\"Skipping t0norm for {group['unique_name'].iloc[0]}\")\n",
    "        return group\n",
    "\n",
    "    filtered_df = filtered_df.groupby(\"unique_name\", group_keys=False).apply(normalize_t0).reset_index(drop=True)\n",
    "    print(\"After t0 normalization:\", filtered_df.shape)\n",
    "\n",
    "    # --- STEP 4: Filter to [nuc_frame - 1, lst] and add COUNT ---\n",
    "    filtered_df = filtered_df[(filtered_df[\"FRAME\"] >= (nuc_frame - 1)) & \n",
    "                              (filtered_df[\"FRAME\"] <= lst)]\n",
    "\n",
    "    filtered_df[\"COUNT\"] = filtered_df[\"FRAME\"] - nuc_frame + 1\n",
    "\n",
    "    # --- STEP 5: Save ---\n",
    "    filename = f\"normalized_spots_{group_name}_{nuc_frame}-{lst}.csv\"\n",
    "    filtered_df.to_csv(os.path.join(output_folder, filename), index=False)\n",
    "\n",
    "    return filtered_df\n",
    "\n",
    "\n",
    "# Normalize for Dox group\n",
    "df_norm_dox = normalize_group(df_subset_lst_dox, \"HeLa-2112_2712-2714_Dox\", nuc_frame, lst, output_folder)\n",
    "\n",
    "# Normalize for Untreated group\n",
    "df_norm_un = normalize_group(df_subset_lst_un, \"Untreated\", nuc_frame, lst, output_folder)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(df_norm_dox.columns.tolist())\n",
    "print(df_norm_un.columns.tolist())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"DOX Averages:\")\n",
    "print(df_norm_dox.groupby(\"COUNT\")[\"MEAN_INTENSITY_CH1_NORM\"].mean())\n",
    "\n",
    "print(\"Untreated Averages:\")\n",
    "print(df_norm_un.groupby(\"COUNT\")[\"MEAN_INTENSITY_CH1_NORM\"].mean())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define colors\n",
    "dox_light = \"lightblue\"\n",
    "dox_dark = \"darkblue\"\n",
    "untreated_light = \"lightcoral\"\n",
    "untreated_dark = \"darkred\"\n",
    "\n",
    "# Define variables and their shorthand names\n",
    "variables = {\n",
    "    \"RADIUS_NORM\": \"radius\",\n",
    "    \"RADIUS\": \"radius-NO-NORM\",\n",
    "    \"MEAN_INTENSITY_CH1_NORM\": \"mean-intensity\",\n",
    "    \"MEAN_INTENSITY_CH1_t0NORM\": \"mean-intensity-t0\",\n",
    "    \"MEAN_INTENSITY_CH1\": \"mean-intensity-NO-NORM\",\n",
    "    \"TOTAL_INTENSITY_CH1_NORM\": \"total-intensity\",\n",
    "    \"AREA_NORM\": \"area\",\n",
    "    \"PERIMETER_NORM\": \"perimeter\",\n",
    "}\n",
    "y_labels = {\n",
    "    \"RADIUS_NORM\": \"R/R0\",\n",
    "    \"RADIUS\": \"R\",\n",
    "    \"MEAN_INTENSITY_CH1_NORM\": \"Mean Intensity (Norm)\",\n",
    "    \"MEAN_INTENSITY_CH1_t0NORM\": \"Mean Intensity/Diffuse Phase\",\n",
    "    \"MEAN_INTENSITY_CH1\": \"Mean Intensity\",\n",
    "    \"TOTAL_INTENSITY_CH1_NORM\": \"Total Intensity (Norm)\",\n",
    "    \"AREA_NORM\": \"A/A0\",\n",
    "    \"PERIMETER_NORM\": \"P/P0\",\n",
    "}\n",
    "\n",
    "# Loop over each variable to create the plots\n",
    "for var, shorthand in variables.items():\n",
    "    ## **Plot 1: Individual Traces + Group Averages**\n",
    "    plt.figure(figsize=(8, 6))\n",
    "\n",
    "    # Plot individual traces for DOX (light color)\n",
    "    for _, group in df_norm_dox.groupby(\"unique_name\"):\n",
    "        plt.plot(group[\"COUNT\"], group[var], color=dox_light, alpha=0.3, linewidth=1)\n",
    "\n",
    "    # Plot individual traces for Untreated (light color)\n",
    "    for _, group in df_norm_un.groupby(\"unique_name\"):\n",
    "        plt.plot(group[\"COUNT\"], group[var], color=untreated_light, alpha=0.3, linewidth=1)\n",
    "\n",
    "    # Compute group averages\n",
    "    dox_avg = df_norm_dox.groupby(\"COUNT\")[var].mean()\n",
    "    untreated_avg = df_norm_un.groupby(\"COUNT\")[var].mean()\n",
    "\n",
    "    # Plot group averages in dark colors\n",
    "    plt.plot(dox_avg.index, dox_avg.values, color=dox_dark, marker=\"o\", linestyle=\"-\", linewidth=2, label=\"Dox (Mean)\")\n",
    "    plt.plot(untreated_avg.index, untreated_avg.values, color=untreated_dark, marker=\"o\", linestyle=\"-\", linewidth=2, label=\"Untreated (Mean)\")\n",
    "\n",
    "    # Labels and legend\n",
    "    plt.xlabel(\"t/t0\")\n",
    "    plt.ylabel(y_labels[var])\n",
    "    plt.title(f\"Individual Traces and Group Averages ({y_labels[var]})\")\n",
    "    plt.legend()\n",
    "    plt.grid(True, linestyle=\"--\", alpha=0.5)\n",
    "\n",
    "    # Save plot\n",
    "    plt.savefig(f\"{output_folder}/{shorthand}-all_norm_spots_{nuc_frame}-{lst}.png\", bbox_inches=\"tight\")\n",
    "\n",
    "    # Show plot\n",
    "    plt.show()\n",
    "\n",
    "    ## **Plot 2: Mean with SEM Error Bars**\n",
    "    plt.figure(figsize=(8, 6))\n",
    "\n",
    "    # Compute SEM (Standard Error of the Mean)\n",
    "    dox_sem = df_norm_dox.groupby(\"COUNT\")[var].sem()\n",
    "    untreated_sem = df_norm_un.groupby(\"COUNT\")[var].sem()\n",
    "\n",
    "    # Plot group means with error bars\n",
    "    plt.errorbar(dox_avg.index, dox_avg.values, yerr=dox_sem.values, fmt=\"o-\", color=dox_dark, capsize=5, label=\"Dox (Mean ± SEM)\")\n",
    "    plt.errorbar(untreated_avg.index, untreated_avg.values, yerr=untreated_sem.values, fmt=\"o-\", color=untreated_dark, capsize=5, label=\"Untreated (Mean ± SEM)\")\n",
    "\n",
    "    # Labels and legend\n",
    "    plt.xlabel(\"t/t0\")\n",
    "    plt.ylabel(y_labels[var])\n",
    "    plt.title(f\"Group Averages with SEM Error Bars ({y_labels[var]})\")\n",
    "    plt.legend()\n",
    "    plt.grid(True, linestyle=\"--\", alpha=0.5)\n",
    "\n",
    "    # Save plot\n",
    "    plt.savefig(f\"{output_folder}/{shorthand}-avg_norm_spots_{nuc_frame}-{lst}.png\", bbox_inches=\"tight\")\n",
    "\n",
    "    # Show plot\n",
    "    plt.show()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tracking-env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
