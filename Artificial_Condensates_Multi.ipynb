{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notebook to deal with artifical condensate data AFTER initial per experiment analysis\n",
    "\n",
    "TO POOL DATA FROM MULTIPLE EXPERIMENTS:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#requirements\n",
    "\n",
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy.stats import ttest_ind\n",
    "from scipy.stats import fisher_exact, chi2_contingency"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "concat_folder = \"/Volumes/mohrlab/mohrlabspace/Nora_Mohr/ANALYSIS/Condensates/250402-250403-250404_frame2-30\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load in all the data frames with the comparisons\n",
    "df1_spots_dox = pd.read_csv(\"/Volumes/holtl02lab/holtl02labspace/Holt_Lab_Members/Nora_Holt/Live_Imaging_Other/250402_HeLa-2112_2712-2714_50nM/Results_TFH_ONLY/output/all_data_spots_Dox.csv\")\n",
    "df1_spots_un = pd.read_csv(\"/Volumes/holtl02lab/holtl02labspace/Holt_Lab_Members/Nora_Holt/Live_Imaging_Other/250402_HeLa-2112_2712-2714_50nM/Results_TFH_ONLY/output/all_data_spots_Untreated.csv\")\n",
    "df2_spots_dox = pd.read_csv(\"/Volumes/holtl02lab/holtl02labspace/Holt_Lab_Members/Nora_Holt/Live_Imaging_Other/250403_HeLa-2112_2712-2714_50nM/Results_allTFH/output/all_data_spots_Dox.csv\")\n",
    "df2_spots_un = pd.read_csv(\"/Volumes/holtl02lab/holtl02labspace/Holt_Lab_Members/Nora_Holt/Live_Imaging_Other/250403_HeLa-2112_2712-2714_50nM/Results_allTFH/output/all_data_spots_Untreated.csv\")\n",
    "df3_spots_dox = pd.read_csv(\"/Volumes/holtl02lab/holtl02labspace/Holt_Lab_Members/Nora_Holt/Live_Imaging_Other/250404_HeLa-2112_2712-2714_50nM/Results_allTFH/output/all_data_spots_Dox.csv\")\n",
    "df3_spots_un = pd.read_csv(\"/Volumes/holtl02lab/holtl02labspace/Holt_Lab_Members/Nora_Holt/Live_Imaging_Other/250404_HeLa-2112_2712-2714_50nM/Results_allTFH/output/all_data_spots_Untreated.csv\")\n",
    "df4_spots_dox = pd.read_csv(\"/Volumes/holtl02lab/holtl02labspace/Holt_Lab_Members/Nora_Holt/Live_Imaging_Other/250228_HeLa-2112_Artificial_Condensates_50nM/Results_TFH_ONLY/output/all_data_spots_Dox.csv\")\n",
    "df4_spots_un = pd.read_csv(\"/Volumes/holtl02lab/holtl02labspace/Holt_Lab_Members/Nora_Holt/Live_Imaging_Other/250228_HeLa-2112_Artificial_Condensates_50nM/Results_TFH_ONLY/output/all_data_spots_Untreated.csv\")\n",
    "\n",
    "df1_tracks_dox = pd.read_csv(\"/Volumes/holtl02lab/holtl02labspace/Holt_Lab_Members/Nora_Holt/Live_Imaging_Other/250402_HeLa-2112_2712-2714_50nM/Results_TFH_ONLY/output/all_data_tracks_Dox.csv\")\n",
    "df1_tracks_un = pd.read_csv(\"/Volumes/holtl02lab/holtl02labspace/Holt_Lab_Members/Nora_Holt/Live_Imaging_Other/250402_HeLa-2112_2712-2714_50nM/Results_TFH_ONLY/output/all_data_tracks_Untreated.csv\")\n",
    "df2_tracks_dox = pd.read_csv(\"/Volumes/holtl02lab/holtl02labspace/Holt_Lab_Members/Nora_Holt/Live_Imaging_Other/250403_HeLa-2112_2712-2714_50nM/Results_allTFH/output/all_data_tracks_Dox.csv\")\n",
    "df2_tracks_un = pd.read_csv(\"/Volumes/holtl02lab/holtl02labspace/Holt_Lab_Members/Nora_Holt/Live_Imaging_Other/250403_HeLa-2112_2712-2714_50nM/Results_allTFH/output/all_data_tracks_Untreated.csv\")\n",
    "df3_tracks_dox = pd.read_csv(\"/Volumes/holtl02lab/holtl02labspace/Holt_Lab_Members/Nora_Holt/Live_Imaging_Other/250404_HeLa-2112_2712-2714_50nM/Results_allTFH/output/all_data_tracks_Dox.csv\")\n",
    "df3_tracks_un = pd.read_csv(\"/Volumes/holtl02lab/holtl02labspace/Holt_Lab_Members/Nora_Holt/Live_Imaging_Other/250404_HeLa-2112_2712-2714_50nM/Results_allTFH/output/all_data_tracks_Untreated.csv\")\n",
    "df4_tracks_dox = pd.read_csv(\"/Volumes/holtl02lab/holtl02labspace/Holt_Lab_Members/Nora_Holt/Live_Imaging_Other/250228_HeLa-2112_Artificial_Condensates_50nM/Results_TFH_ONLY/output/all_data_tracks_Dox.csv\")\n",
    "df4_tracks_un = pd.read_csv(\"/Volumes/holtl02lab/holtl02labspace/Holt_Lab_Members/Nora_Holt/Live_Imaging_Other/250228_HeLa-2112_Artificial_Condensates_50nM/Results_TFH_ONLY/output/all_data_tracks_Untreated.csv\")\n",
    "\n",
    "# Concatenate by group and type\n",
    "all_data_spots_Dox = pd.concat([df1_spots_dox, df2_spots_dox, df3_spots_dox, df4_spots_dox], ignore_index=True)\n",
    "all_data_spots_Untreated = pd.concat([df1_spots_un, df2_spots_un, df3_spots_un, df4_spots_un], ignore_index=True)\n",
    "all_data_tracks_Dox = pd.concat([df1_tracks_dox, df2_tracks_dox, df3_tracks_dox, df4_tracks_dox], ignore_index=True)\n",
    "all_data_tracks_Untreated = pd.concat([df1_tracks_un, df2_tracks_un, df3_tracks_un, df4_tracks_un], ignore_index=True)\n",
    "\n",
    "# Save to CSV\n",
    "all_data_spots_Dox.to_csv(os.path.join(concat_folder, f\"all_data_spots_Dox_combined.csv\"), index=False)\n",
    "all_data_spots_Untreated.to_csv(os.path.join(concat_folder, f\"all_data_spots_Untreated_combined.csv\"), index=False)\n",
    "all_data_tracks_Dox.to_csv(os.path.join(concat_folder, f\"all_data_tracks_Dox_combined.csv\"), index=False)\n",
    "all_data_tracks_Untreated.to_csv(os.path.join(concat_folder, f\"all_data_tracks_Untreated_combined.csv\"), index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read back in, if already exists\n",
    "\n",
    "all_data_spots_Dox = pd.read_csv(os.path.join(concat_folder, f\"all_data_spots_Dox_combined.csv\"), header = 0)\n",
    "all_data_spots_Untreated = pd.read_csv(os.path.join(concat_folder, f\"all_data_spots_Untreated_combined.csv\"), header = 0)\n",
    "all_data_tracks_Dox = pd.read_csv(os.path.join(concat_folder, f\"all_data_tracks_Dox_combined.csv\"), header = 0)\n",
    "all_data_tracks_Untreated = pd.read_csv(os.path.join(concat_folder, f\"all_data_tracks_Untreated_combined.csv\"), header = 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Filter out rows with missing TRACK_INDEX ---\n",
    "tracks_dox_filtered = all_data_tracks_Dox.dropna(subset=['TRACK_INDEX'])\n",
    "tracks_un_filtered = all_data_tracks_Untreated.dropna(subset=['TRACK_INDEX'])\n",
    "\n",
    "# --- Conditionally convert TRACK_START from frames to minutes ---\n",
    "tracks_dox_filtered['TRACK_START'] = tracks_dox_filtered['TRACK_START'].apply(\n",
    "    lambda x: np.floor(x / 60) if x >= 100 else x\n",
    ")\n",
    "tracks_un_filtered['TRACK_START'] = tracks_un_filtered['TRACK_START'].apply(\n",
    "    lambda x: np.floor(x / 60) if x >= 100 else x\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Define 2-minute bins ---\n",
    "bin_width = 2\n",
    "max_bin = max(tracks_dox_filtered['TRACK_START'].max(), tracks_un_filtered['TRACK_START'].max()) + bin_width\n",
    "bins = np.arange(0, max_bin + bin_width, bin_width)\n",
    "\n",
    "# --- Compute histogram counts ---\n",
    "dox_counts, _ = np.histogram(tracks_dox_filtered['TRACK_START'], bins=bins)\n",
    "un_counts, _ = np.histogram(tracks_un_filtered['TRACK_START'], bins=bins)\n",
    "\n",
    "p_values = []\n",
    "for d_count, u_count in zip(dox_counts, un_counts):\n",
    "    # 2x2 contingency table: Dox vs Untreated in this bin vs all other bins\n",
    "    table = [[d_count, u_count],\n",
    "             [sum(dox_counts) - d_count, sum(un_counts) - u_count]]\n",
    "    \n",
    "    # Choose test based on counts\n",
    "    if d_count + u_count < 20:\n",
    "        _, p = fisher_exact(table)\n",
    "    else:\n",
    "        _, p, _, _ = chi2_contingency(table)\n",
    "    \n",
    "    p_values.append(p)\n",
    "\n",
    "# --- Create DataFrame for histogram output ---\n",
    "histogram_df = pd.DataFrame({\n",
    "    \"TIME_BIN_START\": bins[:-1],\n",
    "    \"NUCLEATION_EVENTS_Dox\": dox_counts,\n",
    "    \"NUCLEATION_EVENTS_Untreated\": un_counts\n",
    "})\n",
    "\n",
    "# Save histogram data to CSV\n",
    "histogram_df.to_csv(os.path.join(concat_folder, \"nucleation_histogram_data.csv\"), index=False)\n",
    "\n",
    "# --- Plotting ---\n",
    "plt.figure(figsize=(8, 6))\n",
    "\n",
    "sns.histplot(tracks_dox_filtered['TRACK_START'], bins=bins, color='#40007F', label='Dox', alpha=0.6, kde=False, stat='count')\n",
    "sns.histplot(tracks_un_filtered['TRACK_START'], bins=bins, color='#aa88ff', label='Untreated', alpha=0.6, kde=False, stat='count')\n",
    "\n",
    "# Add stars to bins with significant p-values\n",
    "for i, (bin_start, p) in enumerate(zip(bins[:-1], p_values)):\n",
    "    if p < 0.05:\n",
    "        # Get the max bar height at this bin to position the star\n",
    "        max_count = max(dox_counts[i], un_counts[i])\n",
    "        # Adjust star position slightly above the bar\n",
    "        plt.text(bin_start + bin_width/2, max_count + 0.5, '*', ha='center', va='bottom', fontsize=14, color='black')\n",
    "\n",
    "plt.xlabel(\"Nucleation time (minutes)\")\n",
    "plt.ylabel(\"Track count\")\n",
    "plt.title(\"Histogram of Track Nucleation Times\")\n",
    "plt.legend()\n",
    "plt.grid(True, linestyle=\"--\", alpha=0.5)\n",
    "\n",
    "# Save the figure\n",
    "plt.savefig(os.path.join(concat_folder, \"nucleation_histogram_with_stars.png\"), bbox_inches=\"tight\", dpi=300)\n",
    "plt.show()\n",
    "\n",
    "# Plot the density of 'TRACK_START' for Dox and Untreated groups\n",
    "plt.figure(figsize=(10, 6))\n",
    "\n",
    "# Plot density for Dox group\n",
    "sns.kdeplot(tracks_dox_filtered['TRACK_START'], color='#40007F', label='Dox', shade=True, alpha=0.5)\n",
    "\n",
    "# Plot density for Untreated group\n",
    "sns.kdeplot(tracks_un_filtered['TRACK_START'], color='#aa88ff', label='Untreated', shade=True, alpha=0.5)\n",
    "\n",
    "# Labels and title\n",
    "plt.xlabel(\"Nucleation time (in minutes)\")\n",
    "plt.ylabel(\"Density\")\n",
    "plt.title(\"Density Plot of TRACK_START for Dox and Untreated Groups\")\n",
    "plt.legend()\n",
    "plt.grid(True, linestyle=\"--\", alpha=0.5)\n",
    "\n",
    "plt.savefig(f\"{concat_folder}/nucleation_density.png\", bbox_inches=\"tight\", dpi=300)\n",
    "# Show the plot\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.stats import ks_2samp\n",
    "\n",
    "stat, p = ks_2samp(tracks_dox_filtered['TRACK_START'], tracks_un_filtered['TRACK_START'])\n",
    "print(f\"KS test p-value = {p:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Settings ---\n",
    "first = 0\n",
    "lst = 30  # last frame of interest\n",
    "max_intensity = 550\n",
    "\n",
    "# --- Subset for cells present at last frame (lst) ---\n",
    "def get_present_at_end(df_spots, lst):\n",
    "    unique_names_lst = df_spots[df_spots['FRAME'] == lst]['unique_name']\n",
    "    print(f\"Unique cells present at FRAME {lst}: {unique_names_lst.nunique()}\")\n",
    "    return df_spots[df_spots['unique_name'].isin(unique_names_lst)]\n",
    "\n",
    "# --- Processing function ---\n",
    "def process_and_save_subset(df_subset, group_name, first, lst, concat_folder, max_intensity=550):\n",
    "    # Filter by frame range\n",
    "    df_subset = df_subset[(df_subset[\"FRAME\"] >= (first - 1)) & (df_subset[\"FRAME\"] <= lst)]\n",
    "    \n",
    "    # Filter by max intensity at t0\n",
    "    if max_intensity is not None:\n",
    "        df_subset = df_subset[df_subset[\"Mean_Intensity_t0\"] <= max_intensity]\n",
    "    \n",
    "    # Add COUNT column\n",
    "    df_subset[\"COUNT\"] = df_subset[\"FRAME\"] - first\n",
    "\n",
    "    # Save to CSV\n",
    "    filename = f\"subset_spots_{group_name}_{first}-{lst}.csv\"\n",
    "    output_path = os.path.join(concat_folder, filename)\n",
    "    df_subset.to_csv(output_path, index=False)\n",
    "\n",
    "    return df_subset\n",
    "\n",
    "# --- Apply to both Dox and Untreated ---\n",
    "subset_spots_dox = get_present_at_end(all_data_spots_Dox, lst)\n",
    "subset_spots_un = get_present_at_end(all_data_spots_Untreated, lst)\n",
    "\n",
    "# Filter, count, save\n",
    "df_range_dox = process_and_save_subset(subset_spots_dox, \"Dox\", first, lst, concat_folder)\n",
    "df_range_un = process_and_save_subset(subset_spots_un, \"Untreated\", first, lst, concat_folder)\n",
    "\n",
    "#sanity check for filtration:\n",
    "plt.figure(figsize=(8, 6))\n",
    "\n",
    "# Combine the two filtered DataFrames and add a group column\n",
    "df_range_dox[\"Group\"] = \"Dox\"\n",
    "df_range_un[\"Group\"] = \"Untreated\"\n",
    "df_combined = pd.concat([df_range_dox, df_range_un])\n",
    "\n",
    "# Boxplot\n",
    "sns.boxplot(\n",
    "    data=df_combined,\n",
    "    x=\"Group\",\n",
    "    y=\"Mean_Intensity_t0\",\n",
    "    palette={\"Dox\": \"#7f32bd\", \"Untreated\": \"#aa88ff\"},\n",
    "    showcaps=True,\n",
    "    boxprops={\"facecolor\": \"none\", \"edgecolor\": \"black\"},\n",
    "    whiskerprops={\"color\": \"black\"},\n",
    "    medianprops={\"color\": \"black\"}\n",
    ")\n",
    "\n",
    "# Overlay points\n",
    "sns.stripplot(\n",
    "    data=df_combined,\n",
    "    x=\"Group\",\n",
    "    y=\"Mean_Intensity_t0\",\n",
    "    jitter=True,\n",
    "    alpha=0.5,\n",
    "    palette={\"Dox\": \"#7f32bd\", \"Untreated\": \"#aa88ff\"}\n",
    ")\n",
    "\n",
    "# Labels and title\n",
    "plt.ylabel(\"Mean Intensity t0\")\n",
    "plt.title(\"Mean Intensity of Dilute Phase (t0) by Group\")\n",
    "plt.grid(True, linestyle=\"--\", alpha=0.5)\n",
    "\n",
    "# Save and show\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Updated Settings ---\n",
    "output_folder = concat_folder  # now using concat_folder\n",
    "os.makedirs(output_folder, exist_ok=True)\n",
    "\n",
    "# Output CSV path\n",
    "filename = f\"spots_area_{first}-{lst}.csv\"\n",
    "csv_output_path = os.path.join(output_folder, filename)\n",
    "\n",
    "# Colors\n",
    "dox_dark = \"#7f32bd\"\n",
    "untreated_dark = \"#aa88ff\"\n",
    "\n",
    "# Plotting variables\n",
    "variables = {\n",
    "    \"unique_track_ids\": \"spots-count\",\n",
    "    \"total_area\": \"total-area\",\n",
    "}\n",
    "y_labels = {\n",
    "    \"unique_track_ids\": \"Number of Condensates\",\n",
    "    \"total_area\": \"Total Condensate Area\",\n",
    "}\n",
    "\n",
    "# Initialize list to store CSV output data & stats\n",
    "csv_data = []\n",
    "stat_results = []\n",
    "\n",
    "# --- Loop through each variable ---\n",
    "for var, shorthand in variables.items():\n",
    "    # Group by original_filename + FRAME, then summarize\n",
    "    grouped_dox = df_range_dox.groupby(['original_filename', 'FRAME']).agg(\n",
    "        unique_track_ids=('TRACK_ID', 'nunique'),\n",
    "        total_area=('AREA', 'sum')\n",
    "    ).reset_index()\n",
    "\n",
    "    grouped_un = df_range_un.groupby(['original_filename', 'FRAME']).agg(\n",
    "        unique_track_ids=('TRACK_ID', 'nunique'),\n",
    "        total_area=('AREA', 'sum')\n",
    "    ).reset_index()\n",
    "\n",
    "    # --- Perform statistical test per frame ---\n",
    "    frames = sorted(set(grouped_dox[\"FRAME\"]) & set(grouped_un[\"FRAME\"]))\n",
    "\n",
    "    for frame in frames:\n",
    "        dox_values = grouped_dox[grouped_dox[\"FRAME\"] == frame][var]\n",
    "        un_values = grouped_un[grouped_un[\"FRAME\"] == frame][var]\n",
    "\n",
    "        # Use t-test or Mann–Whitney depending on data distribution\n",
    "        t_stat, t_pval = ttest_ind(dox_values, un_values, equal_var=False, nan_policy=\"omit\")\n",
    "\n",
    "        stat_results.append({\n",
    "            \"variable\": var,\n",
    "            \"FRAME\": frame,\n",
    "            \"t_statistic\": t_stat,\n",
    "            \"p_value\": t_pval,\n",
    "            \"dox_mean\": dox_values.mean(),\n",
    "            \"un_mean\": un_values.mean()\n",
    "        })\n",
    "\n",
    "    # Group-level mean and SEM per frame\n",
    "    dox_avg = grouped_dox.groupby(\"FRAME\").agg(\n",
    "        avg_y=(var, 'mean'),\n",
    "        sem_y=(var, lambda x: np.std(x) / np.sqrt(len(x)))\n",
    "    ).reset_index()\n",
    "\n",
    "    untreated_avg = grouped_un.groupby(\"FRAME\").agg(\n",
    "        avg_y=(var, 'mean'),\n",
    "        sem_y=(var, lambda x: np.std(x) / np.sqrt(len(x)))\n",
    "    ).reset_index()\n",
    "\n",
    "    # --- Plot: Mean ± SEM ---\n",
    "    plt.figure(figsize=(5, 5))\n",
    "\n",
    "    # Plot the mean line + dots (keep these bold)\n",
    "    plt.plot(dox_avg[\"FRAME\"], dox_avg[\"avg_y\"], \"o-\", color=dox_dark, label=\"Dox (Mean ± SEM)\")\n",
    "    plt.plot(untreated_avg[\"FRAME\"], untreated_avg[\"avg_y\"], \"o-\", color=untreated_dark, label=\"Untreated (Mean ± SEM)\")\n",
    "\n",
    "    # Add lighter, thinner error bars separately\n",
    "    plt.errorbar(dox_avg[\"FRAME\"], dox_avg[\"avg_y\"], yerr=dox_avg[\"sem_y\"],\n",
    "            fmt=\"none\", ecolor=dox_dark, elinewidth=1, capsize=3, alpha=0.4)\n",
    "\n",
    "    plt.errorbar(untreated_avg[\"FRAME\"], untreated_avg[\"avg_y\"], yerr=untreated_avg[\"sem_y\"],\n",
    "            fmt=\"none\", ecolor=untreated_dark, elinewidth=1, capsize=3, alpha=0.4)\n",
    "\n",
    "    plt.xlabel(\"t/t0\")\n",
    "    plt.ylabel(y_labels[var])\n",
    "    plt.title(f\"Group Averages with SEM ({y_labels[var]})\")\n",
    "    plt.legend()\n",
    "    plt.grid(False) #This controls the background grid existence (or not)\n",
    "    plt.ylim(bottom=0)\n",
    "\n",
    "    plt.savefig(f\"{output_folder}/{shorthand}-sem_{first}-{lst}.png\", bbox_inches=\"tight\", dpi=300)\n",
    "    plt.show()\n",
    "\n",
    "    # --- Collect data for CSV ---\n",
    "    df_combined = pd.concat([grouped_dox, grouped_un], ignore_index=True)\n",
    "\n",
    "    df_avg = df_combined.groupby(\"FRAME\").agg(\n",
    "        avg_spots_count=('unique_track_ids', 'mean'),\n",
    "        avg_total_area=('total_area', 'mean')\n",
    "    ).reset_index()\n",
    "\n",
    "    df_combined = df_combined.merge(df_avg, on=\"FRAME\", how=\"left\")\n",
    "\n",
    "    df_combined = df_combined[[\n",
    "        \"original_filename\", \"FRAME\", \"unique_track_ids\", \"total_area\",\n",
    "        \"avg_spots_count\", \"avg_total_area\"\n",
    "    ]]\n",
    "\n",
    "    df_combined.columns = [\n",
    "        \"original_filename\", \"FRAME\", \"spots_count\", \"total_area\",\n",
    "        \"avg_spots_count-perframe\", \"avg_total_area-perframe\"\n",
    "    ]\n",
    "\n",
    "    csv_data.append(df_combined)\n",
    "\n",
    "# --- Save stats to CSV ---\n",
    "stats_df = pd.DataFrame(stat_results)\n",
    "stats_output_path = os.path.join(output_folder, f\"stats_{first}-{lst}.csv\")\n",
    "stats_df.to_csv(stats_output_path, index=False)\n",
    "print(f\"Stats file saved: {stats_output_path}\")\n",
    "\n",
    "# --- Save final CSV ---\n",
    "df_final = pd.concat(csv_data, ignore_index=True)\n",
    "df_final.to_csv(csv_output_path, index=False)\n",
    "print(f\"CSV file saved: {csv_output_path}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nuc_frame = 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_and_save_subset(df_subset, group_name, nuc_frame, lst, output_folder):\n",
    "    \"\"\"\n",
    "    Filter dataframe to include only frames from nuc_frame-1 to lst,\n",
    "    compute COUNT relative to nuc_frame, and save to CSV.\n",
    "\n",
    "    Parameters:\n",
    "    - df_subset: Input dataframe (must not be None)\n",
    "    - group_name: Name of the group (e.g., \"Dox\", \"Untreated\")\n",
    "    - nuc_frame: Nucleation frame (used to align timepoints)\n",
    "    - lst: Last frame to include\n",
    "    - output_folder: Folder to save output file\n",
    "\n",
    "    Returns:\n",
    "    - Filtered and annotated dataframe\n",
    "    \"\"\"\n",
    "    if df_subset is None:\n",
    "        raise ValueError(f\"Dataframe for {group_name} is None. Please check your input data.\")\n",
    "\n",
    "    # Debug preview\n",
    "    print(f\"Processing '{group_name}' data:\")\n",
    "    print(df_subset.head())\n",
    "\n",
    "    # Filter the range and compute relative COUNT\n",
    "    df_filtered = df_subset[(df_subset[\"FRAME\"] >= (nuc_frame - 1)) & \n",
    "                            (df_subset[\"FRAME\"] <= lst)].copy()\n",
    "\n",
    "    df_filtered[\"COUNT\"] = df_filtered[\"FRAME\"] - nuc_frame + 1\n",
    "\n",
    "    # Save to CSV\n",
    "    output_path = os.path.join(output_folder, f\"subset_spots_{group_name}_{nuc_frame}-{lst}.csv\")\n",
    "    df_filtered.to_csv(output_path, index=False)\n",
    "\n",
    "    print(f\"Saved filtered data for '{group_name}' to: {output_path}\")\n",
    "    return df_filtered\n",
    "\n",
    "# Apply to both groups\n",
    "df_subset_range_dox = process_and_save_subset(df_range_dox, \"Dox\", nuc_frame, lst, concat_folder)\n",
    "df_subset_range_un = process_and_save_subset(df_range_un, \"Untreated\", nuc_frame, lst, concat_folder)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def normalize_group(df_subset, group_name, nuc_frame, lst, output_folder):\n",
    "    df_subset = df_subset.copy()\n",
    "\n",
    "    # --- STEP 1: Normalize intensity columns to Mean_Intensity_t0 ---\n",
    "    intensity_cols = [\"MEAN_INTENSITY_CH1\", \"MEDIAN_INTENSITY_CH1\", \"TOTAL_INTENSITY_CH1\"]\n",
    "    for col in intensity_cols:\n",
    "        df_subset[f\"{col}_t0NORM\"] = df_subset[col] / df_subset[\"Mean_Intensity_t0\"]\n",
    "\n",
    "    # --- STEP 2: Normalize RADIUS, AREA, PERIMETER to their value at nuc_frame ---\n",
    "    norm_cols_direct = [\"RADIUS\", \"AREA\", \"PERIMETER\"]\n",
    "\n",
    "    # Filter only names present at nuc_frame\n",
    "    valid_names = df_subset[df_subset[\"FRAME\"] == nuc_frame][\"unique_name\"].unique()\n",
    "    filtered_df = df_subset[df_subset[\"unique_name\"].isin(valid_names)].copy()\n",
    "\n",
    "    # Create reference for each unique_name at nuc_frame\n",
    "    ref_direct = filtered_df[filtered_df[\"FRAME\"] == nuc_frame].set_index(\"unique_name\")[norm_cols_direct]\n",
    "\n",
    "    def normalize_direct(group):\n",
    "        name = group[\"unique_name\"].iloc[0]\n",
    "        if name in ref_direct.index:\n",
    "            ref_vals = ref_direct.loc[name]\n",
    "            for col in norm_cols_direct:\n",
    "                group[f\"{col}_NORM\"] = group[col] / ref_vals[col]\n",
    "        return group\n",
    "\n",
    "    filtered_df = filtered_df.groupby(\"unique_name\", group_keys=False).apply(normalize_direct)\n",
    "\n",
    "    # --- STEP 3: Normalize *_t0NORM columns to their value at nuc_frame ---\n",
    "    norm_cols_t0norm = [f\"{col}_t0NORM\" for col in intensity_cols]\n",
    "    ref_t0norm = filtered_df[filtered_df[\"FRAME\"] == nuc_frame].set_index(\"unique_name\")[norm_cols_t0norm]\n",
    "\n",
    "    def normalize_t0(group):\n",
    "        name = group[\"unique_name\"].iloc[0]\n",
    "        if name in ref_t0norm.index:\n",
    "            ref_vals = ref_t0norm.loc[name]\n",
    "            for col in norm_cols_t0norm:\n",
    "                group[col.replace(\"_t0NORM\", \"_NORM\")] = group[col] / ref_vals[col]\n",
    "        return group\n",
    "\n",
    "    filtered_df = filtered_df.groupby(\"unique_name\", group_keys=False).apply(normalize_t0)\n",
    "\n",
    "    # --- STEP 4: Filter range and compute COUNT ---\n",
    "    filtered_df = filtered_df[(filtered_df[\"FRAME\"] >= (nuc_frame - 1)) & \n",
    "                              (filtered_df[\"FRAME\"] <= lst)].copy()\n",
    "    \n",
    "    filtered_df[\"COUNT\"] = filtered_df[\"FRAME\"] - nuc_frame + 1\n",
    "\n",
    "    # --- STEP 5: Save to CSV ---\n",
    "    filename = f\"normalized_spots_{group_name}_{nuc_frame}-{lst}.csv\"\n",
    "    save_path = os.path.join(output_folder, filename)\n",
    "    filtered_df.to_csv(save_path, index=False)\n",
    "\n",
    "    print(f\"Saved normalized dataframe for {group_name} to: {save_path}\")\n",
    "    return filtered_df\n",
    "\n",
    "# Apply to both groups\n",
    "df_norm_dox = normalize_group(df_subset_range_dox, \"Dox\", nuc_frame, lst, concat_folder)\n",
    "df_norm_un = normalize_group(df_subset_range_un, \"Untreated\", nuc_frame, lst, concat_folder)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define colors\n",
    "dox_dark = \"#7f32bd\"\n",
    "untreated_dark = \"#aa88ff\"\n",
    "\n",
    "# Define variables and their shorthand names\n",
    "variables = {\n",
    "    \"RADIUS_NORM\": \"radius\",\n",
    "    \"RADIUS\": \"radius-NO-NORM\",\n",
    "    \"MEAN_INTENSITY_CH1\": \"mean-intensity-NO-NORM\",\n",
    "    \"MEAN_INTENSITY_CH1_NORM\": \"mean-intensity\",\n",
    "    \"TOTAL_INTENSITY_CH1_NORM\": \"total-intensity\",\n",
    "    \"MEAN_INTENSITY_CH1_t0NORM\": \"mean-intensity-t0\",\n",
    "    \"AREA_NORM\": \"area\",\n",
    "    \"PERIMETER_NORM\": \"perimeter\",\n",
    "}\n",
    "y_labels = {\n",
    "    \"RADIUS_NORM\": \"R/R0\",\n",
    "    \"RADIUS\": \"radius\",\n",
    "    \"MEAN_INTENSITY_CH1\": \"Mean Intensity\",\n",
    "    \"MEAN_INTENSITY_CH1_NORM\": \"Mean Intensity (Norm)\",\n",
    "    \"TOTAL_INTENSITY_CH1_NORM\": \"Total Intensity (Norm)\",\n",
    "    \"MEAN_INTENSITY_CH1_t0NORM\": \"Mean Intensity/Diffuse Phase\",\n",
    "    \"AREA_NORM\": \"A/A0\",\n",
    "    \"PERIMETER_NORM\": \"P/P0\",\n",
    "}\n",
    "\n",
    "# Loop over each variable to create the group average + SEM plots\n",
    "for var, shorthand in variables.items():\n",
    "    plt.figure(figsize=(8, 6))\n",
    "\n",
    "    # Group means\n",
    "    dox_avg = df_norm_dox.groupby(\"COUNT\")[var].mean()\n",
    "    untreated_avg = df_norm_un.groupby(\"COUNT\")[var].mean()\n",
    "\n",
    "    # Group SEMs\n",
    "    dox_sem = df_norm_dox.groupby(\"COUNT\")[var].sem()\n",
    "    untreated_sem = df_norm_un.groupby(\"COUNT\")[var].sem()\n",
    "\n",
    "    # Plot with error bars\n",
    "    plt.errorbar(dox_avg.index, dox_avg.values, yerr=dox_sem.values, fmt=\"o-\", color=dox_dark,\n",
    "                 capsize=5, label=\"Dox (Mean ± SEM)\")\n",
    "    plt.errorbar(untreated_avg.index, untreated_avg.values, yerr=untreated_sem.values, fmt=\"o-\",\n",
    "                 color=untreated_dark, capsize=5, label=\"Untreated (Mean ± SEM)\")\n",
    "    \n",
    "    '''# Filter p-values for this variable\n",
    "    sig_data = df_stats[(df_stats[\"variable\"] == var) & (df_stats[\"p_value\"] < 0.05)]\n",
    "\n",
    "    # Overlay p-values where significant\n",
    "    for _, row in sig_data.iterrows():\n",
    "        x = row[\"count\"]\n",
    "        y_max = max(dox_avg.get(x, 0), untreated_avg.get(x, 0))  # get y-position to place marker just above highest group mean\n",
    "        y_err = max(dox_sem.get(x, 0), untreated_sem.get(x, 0))\n",
    "        y_offset = y_max + y_err + 0.05  # add a bit of vertical space\n",
    "        \n",
    "        # Add star and p-value text\n",
    "        plt.text(x, y_offset, \"*\", ha=\"center\", va=\"bottom\", fontsize=14, color=\"black\")'''\n",
    "\n",
    "\n",
    "    # Labels and title\n",
    "    plt.xlabel(\"t/t0\")\n",
    "    plt.ylabel(y_labels[var])\n",
    "    plt.title(f\"Group Averages with SEM ({y_labels[var]})\")\n",
    "    plt.legend()\n",
    "    plt.grid(False)\n",
    "    plt.ylim(bottom=0)\n",
    "\n",
    "    # Save the plot\n",
    "    plt.savefig(f\"{output_folder}/{shorthand}-avg_norm_spots_{nuc_frame}-{lst}.png\", bbox_inches=\"tight\", dpi=300)\n",
    "    plt.show()\n",
    "\n",
    "# Plot the largest condensate area at each timepoint for each group\n",
    "plt.figure(figsize=(8, 6))\n",
    "\n",
    "# Get the largest area at each timepoint\n",
    "dox_max_area = df_norm_dox.groupby(\"COUNT\")[\"AREA\"].max()\n",
    "untreated_max_area = df_norm_un.groupby(\"COUNT\")[\"AREA\"].max()\n",
    "\n",
    "# Plot the max values\n",
    "plt.plot(dox_max_area.index, dox_max_area.values, \"o-\", color=dox_dark, label=\"Dox (Max Condensate Area)\")\n",
    "plt.plot(untreated_max_area.index, untreated_max_area.values, \"o-\", color=untreated_dark, label=\"Untreated (Max Condensate Area)\")\n",
    "\n",
    "# Labels and title\n",
    "plt.xlabel(\"t/t0\")\n",
    "plt.ylabel(\"Max Area\")\n",
    "plt.title(\"Max Condensate Area Over Time\")\n",
    "plt.legend()\n",
    "plt.grid(False)\n",
    "plt.ylim(bottom=0)\n",
    "\n",
    "# Save the plot\n",
    "plt.savefig(f\"{output_folder}/area-largest_{nuc_frame}-{lst}.png\", bbox_inches=\"tight\", dpi=300)\n",
    "plt.show()\n",
    "\n",
    "\n",
    "#To graph probability distribution of size\n",
    "# Set up plot style\n",
    "sns.set(style=\"whitegrid\")\n",
    "\n",
    "# Combine both groups and label them for plotting\n",
    "df_dox_r = df_norm_dox.copy()\n",
    "df_dox_r[\"Group\"] = \"Dox\"\n",
    "\n",
    "df_un_r = df_norm_un.copy()\n",
    "df_un_r[\"Group\"] = \"Untreated\"\n",
    "\n",
    "df_combined = pd.concat([df_dox_r, df_un_r], ignore_index=True)\n",
    "\n",
    "# Loop over each timepoint\n",
    "timepoints = sorted(df_combined[\"COUNT\"].unique())\n",
    "\n",
    "for tp in timepoints:\n",
    "    plt.figure(figsize=(8, 6))\n",
    "    \n",
    "    # Filter for the current timepoint\n",
    "    df_tp = df_combined[df_combined[\"COUNT\"] == tp]\n",
    "\n",
    "    # Plot KDE for RADIUS per group\n",
    "    sns.kdeplot(data=df_tp[df_tp[\"Group\"] == \"Dox\"], x=\"RADIUS\", fill=True, label=\"Dox\", color=dox_dark, alpha=0.5)\n",
    "    sns.kdeplot(data=df_tp[df_tp[\"Group\"] == \"Untreated\"], x=\"RADIUS\", fill=True, label=\"Untreated\", color=untreated_dark, alpha=0.5)\n",
    "\n",
    "    plt.xlabel(\"Radius\")\n",
    "    plt.ylabel(\"Density\")\n",
    "    plt.title(f\"Condensate Radius Distribution at t/t0 = {tp}\")\n",
    "    plt.legend()\n",
    "    plt.grid(False)\n",
    "    plt.ylim(bottom=0)\n",
    "\n",
    "    # Save the figure\n",
    "    plt.savefig(f\"{output_folder}/radius_distribution_t{tp}_{nuc_frame}-{lst}.png\", bbox_inches=\"tight\", dpi=300)\n",
    "    \n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize results list\n",
    "stats_results = []\n",
    "\n",
    "# Loop through each variable\n",
    "for var, shorthand in variables.items():\n",
    "    print(f\"\\n--- Stats for {var} ---\")\n",
    "    \n",
    "    # Loop through each COUNT value\n",
    "    for count_val in sorted(df_norm_dox[\"COUNT\"].unique()):\n",
    "        dox_vals = df_norm_dox[df_norm_dox[\"COUNT\"] == count_val][var].dropna()\n",
    "        un_vals = df_norm_un[df_norm_un[\"COUNT\"] == count_val][var].dropna()\n",
    "\n",
    "        if len(dox_vals) > 1 and len(un_vals) > 1:\n",
    "            stat, pval = ttest_ind(dox_vals, un_vals, equal_var=False)\n",
    "            stats_results.append({\n",
    "                \"variable\": var,\n",
    "                \"count\": count_val,\n",
    "                \"dox_mean\": dox_vals.mean(),\n",
    "                \"untreated_mean\": un_vals.mean(),\n",
    "                \"t_statistic\": stat,\n",
    "                \"p_value\": pval\n",
    "            })\n",
    "        else:\n",
    "            stats_results.append({\n",
    "                \"variable\": var,\n",
    "                \"count\": count_val,\n",
    "                \"dox_mean\": dox_vals.mean() if len(dox_vals) > 0 else None,\n",
    "                \"untreated_mean\": un_vals.mean() if len(un_vals) > 0 else None,\n",
    "                \"t_statistic\": None,\n",
    "                \"p_value\": None\n",
    "            })\n",
    "\n",
    "# Convert to DataFrame\n",
    "df_stats = pd.DataFrame(stats_results)\n",
    "\n",
    "# Save to CSV\n",
    "df_stats.to_csv(f\"{output_folder}/statistical_comparisons_{nuc_frame}-{lst}.csv\", index=False)\n",
    "\n",
    "print(\"Statistical results saved.\")\n",
    "\n",
    "df_stats"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tracking-env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
